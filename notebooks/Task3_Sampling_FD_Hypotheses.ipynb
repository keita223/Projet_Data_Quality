{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 : Ã‰chantillonnage et HypothÃ¨ses FD\n",
    "\n",
    "## Objectif\n",
    "Ã‰tudier comment les dÃ©pendances fonctionnelles suggÃ©rÃ©es par un LLM sur des **Ã©chantillons** peuvent diffÃ©rer de celles qui tiennent sur le **dataset complet**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de Claude (Anthropic)\n",
    "import anthropic\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "# Force le rechargement\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# RÃ©cupÃ©rer la clÃ© API Claude depuis les variables d'environnement\n",
    "CLAUDE_API_KEY = os.getenv(\"CLAUDE_API_KEY\")\n",
    "\n",
    "# Debug : afficher les premiers et derniers caractÃ¨res de la clÃ©\n",
    "if CLAUDE_API_KEY:\n",
    "    print(f\"ðŸ”‘ ClÃ© chargÃ©e : {CLAUDE_API_KEY[:15]}...{CLAUDE_API_KEY[-5:]}\")\n",
    "    print(f\"ðŸ“ Longueur de la clÃ© : {len(CLAUDE_API_KEY)} caractÃ¨res\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"âŒ ClÃ© API Claude introuvable.\\n\"\n",
    "        \"âž¡ï¸ VÃ©rifie que :\\n\"\n",
    "        \"1. Le fichier .env existe\\n\"\n",
    "        \"2. Il contient la ligne : CLAUDE_API_KEY=ta_cle_api\\n\"\n",
    "        \"3. Le fichier .env est bien chargÃ©\"\n",
    "    )\n",
    "\n",
    "# Configurer le client Claude\n",
    "client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
    "\n",
    "# Test rapide de la connexion\n",
    "try:\n",
    "    test_response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=10,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Dis juste 'OK'\"}]\n",
    "    )\n",
    "    print(f\"âœ… Claude configurÃ© avec succÃ¨s ! Test: {test_response.content[0].text}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur de connexion : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les datasets\n",
    "datasets = {}\n",
    "\n",
    "# IRIS\n",
    "iris_path = '../Datasets/iris/iris.data'\n",
    "if os.path.exists(iris_path):\n",
    "    datasets['iris'] = pd.read_csv(iris_path, header=None, \n",
    "                                    names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n",
    "    print(f\"âœ… iris: {datasets['iris'].shape}\")\n",
    "else:\n",
    "    print(f\"âŒ iris non trouvÃ©: {iris_path}\")\n",
    "\n",
    "# BRIDGES\n",
    "bridges_path = '../Datasets/pittsburgh+bridges/bridges.data.version1'\n",
    "if os.path.exists(bridges_path):\n",
    "    datasets['bridges'] = pd.read_csv(bridges_path, header=None,\n",
    "                                       names=['IDENTIF', 'RIVER', 'LOCATION', 'ERECTED', 'PURPOSE', \n",
    "                                              'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL',\n",
    "                                              'SPAN', 'REL-L', 'TYPE'])\n",
    "    print(f\"âœ… bridges: {datasets['bridges'].shape}\")\n",
    "else:\n",
    "    print(f\"âŒ bridges non trouvÃ©: {bridges_path}\")\n",
    "\n",
    "# ABALONE\n",
    "abalone_path = '../Datasets/abalone/abalone.data'\n",
    "if os.path.exists(abalone_path):\n",
    "    datasets['abalone'] = pd.read_csv(abalone_path, header=None,\n",
    "                                       names=['Sex', 'Length', 'Diameter', 'Height', 'Whole_weight',\n",
    "                                              'Shucked_weight', 'Viscera_weight', 'Shell_weight', 'Rings'])\n",
    "    print(f\"âœ… abalone: {datasets['abalone'].shape}\")\n",
    "else:\n",
    "    print(f\"âŒ abalone non trouvÃ©: {abalone_path}\")\n",
    "\n",
    "# BREAST CANCER\n",
    "bc_path = '../Datasets/breast+cancer+wisconsin+original/breast-cancer-wisconsin.data'\n",
    "if os.path.exists(bc_path):\n",
    "    datasets['breast-cancer'] = pd.read_csv(bc_path, header=None,\n",
    "                                             names=['id', 'clump_thickness', 'uniformity_cell_size',\n",
    "                                                    'uniformity_cell_shape', 'marginal_adhesion',\n",
    "                                                    'single_epithelial_cell_size', 'bare_nuclei',\n",
    "                                                    'bland_chromatin', 'normal_nucleoli', 'mitoses', 'class'])\n",
    "    print(f\"âœ… breast-cancer: {datasets['breast-cancer'].shape}\")\n",
    "else:\n",
    "    print(f\"âŒ breast-cancer non trouvÃ©: {bc_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Datasets chargÃ©s: {list(datasets.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fonctions d'Ã©chantillonnage\n",
    "\n",
    "Nous allons crÃ©er 3 types d'Ã©chantillons :\n",
    "- **AlÃ©atoire** : sÃ©lection au hasard\n",
    "- **StratifiÃ©** : garde les proportions de chaque groupe\n",
    "- **BiaisÃ©** : sur-reprÃ©sente une catÃ©gorie (pour montrer l'impact du biais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(df, n=50, seed=42):\n",
    "    \"\"\"\n",
    "    Ã‰chantillon alÃ©atoire simple.\n",
    "    \"\"\"\n",
    "    n = min(n, len(df))\n",
    "    return df.sample(n=n, random_state=seed)\n",
    "\n",
    "def stratified_sample(df, stratify_col, n=50, seed=42):\n",
    "    \"\"\"\n",
    "    Ã‰chantillon stratifiÃ© basÃ© sur une colonne.\n",
    "    Garde les proportions de chaque groupe.\n",
    "    \"\"\"\n",
    "    n = min(n, len(df))\n",
    "    # Calculer les proportions\n",
    "    proportions = df[stratify_col].value_counts(normalize=True)\n",
    "    \n",
    "    samples = []\n",
    "    for value, prop in proportions.items():\n",
    "        group = df[df[stratify_col] == value]\n",
    "        group_n = max(1, int(n * prop))  # Au moins 1 par groupe\n",
    "        group_n = min(group_n, len(group))\n",
    "        samples.append(group.sample(n=group_n, random_state=seed))\n",
    "    \n",
    "    result = pd.concat(samples)\n",
    "    # Ajuster si on a trop ou pas assez\n",
    "    if len(result) > n:\n",
    "        result = result.sample(n=n, random_state=seed)\n",
    "    return result\n",
    "\n",
    "def biased_sample(df, bias_col, bias_value, n=50, seed=42):\n",
    "    \"\"\"\n",
    "    Ã‰chantillon biaisÃ© : sur-reprÃ©sente une valeur particuliÃ¨re.\n",
    "    \"\"\"\n",
    "    n = min(n, len(df))\n",
    "    biased = df[df[bias_col] == bias_value]\n",
    "    others = df[df[bias_col] != bias_value]\n",
    "    \n",
    "    # 70% du groupe biaisÃ©, 30% des autres\n",
    "    n_biased = min(int(n * 0.7), len(biased))\n",
    "    n_others = min(n - n_biased, len(others))\n",
    "    \n",
    "    sample_biased = biased.sample(n=n_biased, random_state=seed) if n_biased > 0 else pd.DataFrame()\n",
    "    sample_others = others.sample(n=n_others, random_state=seed) if n_others > 0 else pd.DataFrame()\n",
    "    \n",
    "    return pd.concat([sample_biased, sample_others])\n",
    "\n",
    "print(\"âœ… Fonctions d'Ã©chantillonnage dÃ©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CrÃ©er les Ã©chantillons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les Ã©chantillons pour chaque dataset\n",
    "samples = {}\n",
    "\n",
    "# IRIS - Ã©chantillon alÃ©atoire et stratifiÃ© par classe\n",
    "if 'iris' in datasets:\n",
    "    df = datasets['iris']\n",
    "    samples['iris'] = {\n",
    "        'random': random_sample(df, n=30),\n",
    "        'stratified': stratified_sample(df, 'class', n=30)\n",
    "    }\n",
    "    print(f\"âœ… Ã‰chantillons iris crÃ©Ã©s (colonne stratifiÃ©e: class)\")\n",
    "\n",
    "# BRIDGES - Ã©chantillon alÃ©atoire et stratifiÃ© par MATERIAL\n",
    "if 'bridges' in datasets:\n",
    "    df = datasets['bridges']\n",
    "    samples['bridges'] = {\n",
    "        'random': random_sample(df, n=40),\n",
    "        'stratified': stratified_sample(df, 'MATERIAL', n=40)\n",
    "    }\n",
    "    print(f\"âœ… Ã‰chantillons bridges crÃ©Ã©s (colonne stratifiÃ©e: MATERIAL)\")\n",
    "\n",
    "# ABALONE - Ã©chantillon alÃ©atoire et stratifiÃ© par Sex\n",
    "if 'abalone' in datasets:\n",
    "    df = datasets['abalone']\n",
    "    samples['abalone'] = {\n",
    "        'random': random_sample(df, n=50),\n",
    "        'stratified': stratified_sample(df, 'Sex', n=50)\n",
    "    }\n",
    "    print(f\"âœ… Ã‰chantillons abalone crÃ©Ã©s (colonne stratifiÃ©e: Sex)\")\n",
    "\n",
    "# BREAST CANCER - Ã©chantillon alÃ©atoire et stratifiÃ© par class\n",
    "if 'breast-cancer' in datasets:\n",
    "    df = datasets['breast-cancer']\n",
    "    samples['breast-cancer'] = {\n",
    "        'random': random_sample(df, n=50),\n",
    "        'stratified': stratified_sample(df, 'class', n=50)\n",
    "    }\n",
    "    print(f\"âœ… Ã‰chantillons breast-cancer crÃ©Ã©s (colonne stratifiÃ©e: class)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Ã‰chantillons crÃ©Ã©s pour: {list(samples.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher un aperÃ§u des Ã©chantillons\n",
    "for dataset_name, dataset_samples in samples.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for sample_type, sample_df in dataset_samples.items():\n",
    "        print(f\"\\n--- {sample_type.upper()} ({len(sample_df)} lignes) ---\")\n",
    "        display(sample_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonction pour interroger le LLM sur les FDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm_for_fds(sample_df, dataset_name, sample_type):\n",
    "    \"\"\"\n",
    "    Montre un Ã©chantillon au LLM (Claude) et lui demande de suggÃ©rer des FDs.\n",
    "    \"\"\"\n",
    "    # Convertir l'Ã©chantillon en texte (max 20 lignes pour le prompt)\n",
    "    sample_text = sample_df.head(20).to_string(index=False)\n",
    "    columns = list(sample_df.columns)\n",
    "    \n",
    "    prompt = f\"\"\"Tu es un expert en bases de donnÃ©es. Voici un Ã©chantillon de donnÃ©es du dataset \"{dataset_name}\".\n",
    "\n",
    "Colonnes: {columns}\n",
    "\n",
    "Ã‰chantillon ({len(sample_df)} lignes, voici les 20 premiÃ¨res):\n",
    "{sample_text}\n",
    "\n",
    "En analysant CET Ã‰CHANTILLON UNIQUEMENT, quelles dÃ©pendances fonctionnelles (FDs) semblent tenir ?\n",
    "\n",
    "Une FD X â†’ Y signifie : si deux lignes ont la mÃªme valeur pour X, elles ont la mÃªme valeur pour Y.\n",
    "\n",
    "Liste exactement 5 FDs que tu penses vraies dans cet Ã©chantillon.\n",
    "Format de rÃ©ponse (une FD par ligne):\n",
    "FD1: colonne1 -> colonne2\n",
    "FD2: colonne1, colonne2 -> colonne3\n",
    "etc.\n",
    "\n",
    "Ne donne QUE les FDs, pas d'explications.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    except Exception as e:\n",
    "        return f\"ERREUR: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Fonction ask_llm_for_fds() dÃ©finie (utilise Claude 3 Haiku)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interroger le LLM pour chaque Ã©chantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Collecter les FDs suggÃ©rÃ©es par le LLM\n",
    "llm_suggested_fds = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INTERROGATION DU LLM POUR SUGGÃ‰RER DES FDs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_name, dataset_samples in samples.items():\n",
    "    llm_suggested_fds[dataset_name] = {}\n",
    "    \n",
    "    for sample_type, sample_df in dataset_samples.items():\n",
    "        print(f\"\\nðŸ”„ {dataset_name} - {sample_type}...\")\n",
    "        \n",
    "        response = ask_llm_for_fds(sample_df, dataset_name, sample_type)\n",
    "        llm_suggested_fds[dataset_name][sample_type] = response\n",
    "        \n",
    "        print(f\"ðŸ“ RÃ©ponse du LLM:\")\n",
    "        print(response)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        time.sleep(2)  # Pause pour l'API\n",
    "\n",
    "print(\"\\nâœ… Toutes les suggestions collectÃ©es !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parser les FDs suggÃ©rÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_fd_response(response_text):\n",
    "    \"\"\"\n",
    "    Extrait les FDs de la rÃ©ponse du LLM.\n",
    "    Format attendu: \"colA, colB -> colC\" ou \"colA -> colB\"\n",
    "    \"\"\"\n",
    "    fds = []\n",
    "    lines = response_text.strip().split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        # Chercher le pattern \"X -> Y\" ou \"X â†’ Y\"\n",
    "        match = re.search(r'([^->â†’]+)\\s*[-â†’>]+\\s*([^->â†’]+)', line)\n",
    "        if match:\n",
    "            lhs = match.group(1).strip()\n",
    "            rhs = match.group(2).strip()\n",
    "            \n",
    "            # Nettoyer (enlever \"FD1:\", etc.)\n",
    "            lhs = re.sub(r'^FD\\d*:\\s*', '', lhs)\n",
    "            \n",
    "            # Parser le LHS (peut Ãªtre \"col1, col2\")\n",
    "            lhs_cols = [c.strip() for c in lhs.split(',')]\n",
    "            rhs_col = rhs.strip()\n",
    "            \n",
    "            fds.append({\n",
    "                'lhs': lhs_cols,\n",
    "                'rhs': rhs_col,\n",
    "                'fd_string': f\"{', '.join(lhs_cols)} -> {rhs_col}\"\n",
    "            })\n",
    "    \n",
    "    return fds\n",
    "\n",
    "# Parser toutes les rÃ©ponses\n",
    "parsed_fds = {}\n",
    "for dataset_name, dataset_responses in llm_suggested_fds.items():\n",
    "    parsed_fds[dataset_name] = {}\n",
    "    for sample_type, response in dataset_responses.items():\n",
    "        parsed_fds[dataset_name][sample_type] = parse_fd_response(response)\n",
    "\n",
    "# Afficher les FDs parsÃ©es\n",
    "print(\"FDs suggÃ©rÃ©es par le LLM:\")\n",
    "for dataset_name, dataset_fds in parsed_fds.items():\n",
    "    print(f\"\\n{dataset_name.upper()}:\")\n",
    "    for sample_type, fds in dataset_fds.items():\n",
    "        print(f\"  {sample_type}: {len(fds)} FDs\")\n",
    "        for fd in fds:\n",
    "            print(f\"    - {fd['fd_string']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fonction de vÃ©rification des FDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fd_holds(df, lhs_cols, rhs_col):\n",
    "    \"\"\"\n",
    "    VÃ©rifie si une FD tient dans un DataFrame.\n",
    "    Retourne: (holds: bool, violations: int, total_groups: int)\n",
    "    \"\"\"\n",
    "    # VÃ©rifier que les colonnes existent\n",
    "    for col in lhs_cols + [rhs_col]:\n",
    "        if col not in df.columns:\n",
    "            return None, 0, 0  # Colonne non trouvÃ©e\n",
    "    \n",
    "    # Grouper par LHS et compter les valeurs uniques de RHS\n",
    "    grouped = df.groupby(lhs_cols)[rhs_col].nunique()\n",
    "    \n",
    "    # Une FD tient si chaque groupe a exactement 1 valeur pour RHS\n",
    "    violations = (grouped > 1).sum()\n",
    "    total_groups = len(grouped)\n",
    "    holds = violations == 0\n",
    "    \n",
    "    return holds, violations, total_groups\n",
    "\n",
    "def check_fd_approximate(df, lhs_cols, rhs_col):\n",
    "    \"\"\"\n",
    "    Calcule le taux de validitÃ© approximatif d'une FD.\n",
    "    Retourne le pourcentage de groupes qui respectent la FD.\n",
    "    \"\"\"\n",
    "    for col in lhs_cols + [rhs_col]:\n",
    "        if col not in df.columns:\n",
    "            return None\n",
    "    \n",
    "    grouped = df.groupby(lhs_cols)[rhs_col].nunique()\n",
    "    valid_groups = (grouped == 1).sum()\n",
    "    total_groups = len(grouped)\n",
    "    \n",
    "    if total_groups == 0:\n",
    "        return 0\n",
    "    \n",
    "    return (valid_groups / total_groups) * 100\n",
    "\n",
    "print(\"âœ… Fonctions de vÃ©rification dÃ©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Valider les FDs sur Ã©chantillons vs dataset complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valider chaque FD suggÃ©rÃ©e\n",
    "validation_results = []\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"VALIDATION DES FDs SUGGÃ‰RÃ‰ES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for dataset_name, dataset_fds in parsed_fds.items():\n",
    "    if dataset_name not in datasets:\n",
    "        continue\n",
    "    \n",
    "    full_df = datasets[dataset_name]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {dataset_name.upper()} ({len(full_df)} lignes)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for sample_type, fds in dataset_fds.items():\n",
    "        sample_df = samples[dataset_name][sample_type]\n",
    "        \n",
    "        print(f\"\\n--- FDs du {sample_type} ({len(sample_df)} lignes) ---\")\n",
    "        \n",
    "        for fd in fds:\n",
    "            lhs = fd['lhs']\n",
    "            rhs = fd['rhs']\n",
    "            fd_str = fd['fd_string']\n",
    "            \n",
    "            # VÃ©rifier sur l'Ã©chantillon\n",
    "            holds_sample, viol_sample, groups_sample = check_fd_holds(sample_df, lhs, rhs)\n",
    "            \n",
    "            # VÃ©rifier sur le dataset complet\n",
    "            holds_full, viol_full, groups_full = check_fd_holds(full_df, lhs, rhs)\n",
    "            \n",
    "            # Calculer la validitÃ© approximative\n",
    "            approx_sample = check_fd_approximate(sample_df, lhs, rhs)\n",
    "            approx_full = check_fd_approximate(full_df, lhs, rhs)\n",
    "            \n",
    "            if holds_sample is None or holds_full is None:\n",
    "                print(f\"  âš ï¸ {fd_str} - Colonnes non trouvÃ©es\")\n",
    "                continue\n",
    "            \n",
    "            # DÃ©terminer le statut\n",
    "            if holds_sample and holds_full:\n",
    "                status = \"âœ… VRAIE (Ã©chantillon ET complet)\"\n",
    "                category = \"true_positive\"\n",
    "            elif holds_sample and not holds_full:\n",
    "                status = \"âŒ FAUX POSITIF (vraie sur Ã©chantillon, fausse sur complet)\"\n",
    "                category = \"false_positive\"\n",
    "            elif not holds_sample and holds_full:\n",
    "                status = \"ðŸ”¶ Fausse sur Ã©chantillon mais vraie sur complet\"\n",
    "                category = \"sample_error\"\n",
    "            else:\n",
    "                status = \"âŒ FAUSSE (Ã©chantillon ET complet)\"\n",
    "                category = \"false_negative\"\n",
    "            \n",
    "            print(f\"\\n  ðŸ“Œ {fd_str}\")\n",
    "            print(f\"     Ã‰chantillon: {holds_sample} ({approx_sample:.1f}% valide, {viol_sample} violations)\")\n",
    "            print(f\"     Complet: {holds_full} ({approx_full:.1f}% valide, {viol_full} violations)\")\n",
    "            print(f\"     {status}\")\n",
    "            \n",
    "            validation_results.append({\n",
    "                'dataset': dataset_name,\n",
    "                'sample_type': sample_type,\n",
    "                'fd': fd_str,\n",
    "                'holds_sample': holds_sample,\n",
    "                'holds_full': holds_full,\n",
    "                'approx_sample': approx_sample,\n",
    "                'approx_full': approx_full,\n",
    "                'violations_full': viol_full,\n",
    "                'category': category\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tableau rÃ©capitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er le tableau rÃ©capitulatif\n",
    "if validation_results:\n",
    "    results_df = pd.DataFrame(validation_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TABLEAU RÃ‰CAPITULATIF\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    display(results_df[['dataset', 'sample_type', 'fd', 'holds_sample', 'holds_full', 'category']])\n",
    "    \n",
    "    # Statistiques\n",
    "    print(\"\\nðŸ“Š STATISTIQUES:\")\n",
    "    print(f\"   Total FDs analysÃ©es: {len(results_df)}\")\n",
    "    print(f\"   âœ… Vraies positives: {(results_df['category'] == 'true_positive').sum()}\")\n",
    "    print(f\"   âŒ Faux positifs: {(results_df['category'] == 'false_positive').sum()}\")\n",
    "    print(f\"   âŒ Fausses: {(results_df['category'] == 'false_negative').sum()}\")\n",
    "else:\n",
    "    print(\"Aucun rÃ©sultat Ã  afficher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyse des Faux Positifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser les faux positifs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE DES FAUX POSITIFS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCes FDs semblaient vraies sur l'Ã©chantillon mais sont fausses sur le dataset complet.\")\n",
    "print(\"Cela montre que l'Ã©chantillonnage cache des violations !\\n\")\n",
    "\n",
    "if validation_results:\n",
    "    false_positives = [r for r in validation_results if r['category'] == 'false_positive']\n",
    "    \n",
    "    if false_positives:\n",
    "        for i, fp in enumerate(false_positives, 1):\n",
    "            print(f\"\\n--- Faux Positif {i} ---\")\n",
    "            print(f\"Dataset: {fp['dataset']}\")\n",
    "            print(f\"Type d'Ã©chantillon: {fp['sample_type']}\")\n",
    "            print(f\"FD: {fp['fd']}\")\n",
    "            print(f\"ValiditÃ© sur Ã©chantillon: {fp['approx_sample']:.1f}%\")\n",
    "            print(f\"ValiditÃ© sur complet: {fp['approx_full']:.1f}%\")\n",
    "            print(f\"Violations sur dataset complet: {fp['violations_full']}\")\n",
    "    else:\n",
    "        print(\"Aucun faux positif dÃ©tectÃ©.\")\n",
    "else:\n",
    "    print(\"Aucun rÃ©sultat disponible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarder les rÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les rÃ©sultats\n",
    "if validation_results:\n",
    "    results_df = pd.DataFrame(validation_results)\n",
    "    results_df.to_csv('../results/task3_validation_results.csv', index=False)\n",
    "    print(\"âœ… RÃ©sultats sauvegardÃ©s: results/task3_validation_results.csv\")\n",
    "\n",
    "# Sauvegarder les rÃ©ponses LLM\n",
    "with open('../results/task3_llm_responses.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(llm_suggested_fds, f, indent=2, ensure_ascii=False)\n",
    "print(\"âœ… RÃ©ponses LLM: results/task3_llm_responses.json\")\n",
    "\n",
    "# Sauvegarder les FDs parsÃ©es\n",
    "parsed_fds_serializable = {}\n",
    "for dataset, samples_dict in parsed_fds.items():\n",
    "    parsed_fds_serializable[dataset] = {}\n",
    "    for sample_type, fds in samples_dict.items():\n",
    "        parsed_fds_serializable[dataset][sample_type] = fds\n",
    "\n",
    "with open('../results/task3_parsed_fds.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(parsed_fds_serializable, f, indent=2, ensure_ascii=False)\n",
    "print(\"âœ… FDs parsÃ©es: results/task3_parsed_fds.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "### Ce qu'on a appris :\n",
    "\n",
    "1. **L'Ã©chantillonnage cache des violations**\n",
    "   - Une FD peut sembler vraie sur un petit Ã©chantillon\n",
    "   - Mais Ãªtre fausse sur le dataset complet\n",
    "\n",
    "2. **Les LLMs gÃ©nÃ©ralisent trop vite**\n",
    "   - Ils trouvent des patterns dans peu de donnÃ©es\n",
    "   - Ces patterns ne sont pas forcÃ©ment des rÃ¨gles gÃ©nÃ©rales\n",
    "\n",
    "3. **Types de problÃ¨mes dÃ©tectÃ©s :**\n",
    "   - **Faux positifs** : FDs vraies sur Ã©chantillon, fausses sur complet\n",
    "   - **FDs non minimales** : Trop d'attributs dans le LHS\n",
    "   - **FDs trompeuses** : Semblent logiques mais sont accidentelles\n",
    "\n",
    "### LeÃ§on clÃ© :\n",
    "> Les patterns empiriques sur des Ã©chantillons ne sont PAS des contraintes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
