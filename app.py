"""
Application Streamlit - Projet Qualit√© des Donn√©es
D√©couverte de D√©pendances Fonctionnelles avec Algorithmes + LLM
M2 IASD - Paris Dauphine
"""

import streamlit as st
import pandas as pd
import numpy as np
import json
import os
from itertools import combinations
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Configuration de la page
st.set_page_config(
    page_title="FD Discovery - Projet Data Quality",
    page_icon="üîç",
    layout="wide"
)

# Titre principal
st.title("üîç D√©couverte de D√©pendances Fonctionnelles")
st.markdown("### Approche Hybride : Algorithmes + LLM")
st.markdown("**M2 IASD - Universit√© Paris-Dauphine**")
st.markdown("---")

# Sidebar pour la navigation
st.sidebar.title("Navigation")
page = st.sidebar.radio(
    "Choisir une section :",
    ["üè† Accueil",
     "üìä Task 1: D√©couverte Algorithmique",
     "ü§ñ Task 2: Analyse LLM",
     "üìâ Task 3: √âchantillonnage",
     "üîÑ Task 4: Pipeline Hybride",
     "üìà R√©sultats Finaux"]
)

# ============================================
# Fonctions utilitaires
# ============================================

@st.cache_data
def load_datasets():
    """Charge tous les datasets"""
    datasets = {}

    # IRIS
    iris_path = 'Datasets/iris/iris.data'
    if os.path.exists(iris_path):
        datasets['iris'] = pd.read_csv(iris_path, header=None,
                                        names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])

    # BRIDGES
    bridges_path = 'Datasets/pittsburgh+bridges/bridges.data.version1'
    if os.path.exists(bridges_path):
        datasets['bridges'] = pd.read_csv(bridges_path, header=None,
                                           names=['IDENTIF', 'RIVER', 'LOCATION', 'ERECTED', 'PURPOSE',
                                                  'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL',
                                                  'SPAN', 'REL-L', 'TYPE'])

    # ABALONE
    abalone_path = 'Datasets/abalone/abalone.data'
    if os.path.exists(abalone_path):
        datasets['abalone'] = pd.read_csv(abalone_path, header=None,
                                           names=['Sex', 'Length', 'Diameter', 'Height', 'Whole_weight',
                                                  'Shucked_weight', 'Viscera_weight', 'Shell_weight', 'Rings'])

    # NURSERY
    nursery_path = 'Datasets/nursery/nursery.data'
    if os.path.exists(nursery_path):
        datasets['nursery'] = pd.read_csv(nursery_path, header=None,
                                           names=['parents', 'has_nurs', 'form', 'children',
                                                  'housing', 'finance', 'social', 'health', 'class'])

    return datasets

def check_fd(df, lhs_cols, rhs_col):
    """V√©rifie si une FD tient"""
    if isinstance(lhs_cols, str):
        lhs_cols = [lhs_cols]

    grouped = df.groupby(list(lhs_cols))[rhs_col].nunique()
    violations = (grouped > 1).sum()
    total_groups = len(grouped)

    if total_groups == 0:
        return False, 0, 0

    validity_rate = ((total_groups - violations) / total_groups) * 100
    holds = violations == 0

    return holds, validity_rate, violations

def discover_fds(df, min_validity=90, max_lhs_size=2):
    """D√©couvre les FDs"""
    columns = list(df.columns)
    fds = []

    for lhs_size in range(1, max_lhs_size + 1):
        for lhs_cols in combinations(columns, lhs_size):
            lhs_cols = list(lhs_cols)
            for rhs_col in columns:
                if rhs_col in lhs_cols:
                    continue
                holds, validity, violations = check_fd(df, lhs_cols, rhs_col)
                if validity >= min_validity:
                    fds.append({
                        'lhs': ', '.join(lhs_cols),
                        'rhs': rhs_col,
                        'fd_string': f"{', '.join(lhs_cols)} ‚Üí {rhs_col}",
                        'validity': validity,
                        'violations': violations
                    })

    return fds

# ============================================
# Pages
# ============================================

if page == "üè† Accueil":
    st.header("Bienvenue dans le Projet Qualit√© des Donn√©es")

    st.markdown("""
    ## üéØ Objectif du Projet

    Combiner les **algorithmes classiques** de d√©couverte de d√©pendances fonctionnelles
    avec les **Large Language Models (LLMs)** pour identifier les FDs **significatives**.

    ## üìã Les 4 T√¢ches
    """)

    col1, col2 = st.columns(2)

    with col1:
        st.info("""
        **Task 1 : D√©couverte Algorithmique**
        - Algorithme de d√©couverte de FDs
        - Calcul de validit√©
        - FDs exactes et approximatives
        """)

        st.success("""
        **Task 2 : Analyse S√©mantique LLM**
        - √âvaluation par Claude 3 Haiku
        - Cat√©gorisation : key, business_rule, derived, accidental
        - Score s√©mantique 0-10
        """)

    with col2:
        st.warning("""
        **Task 3 : √âchantillonnage**
        - √âchantillons al√©atoires vs stratifi√©s
        - D√©tection des faux positifs
        - **9% de faux positifs d√©tect√©s !**
        """)

        st.error("""
        **Task 4 : Pipeline Hybride**
        - Combinaison Algorithme + LLM
        - Score hybride = (technique + s√©mantique) / 2
        - **R√©duction de 96% du volume**
        """)

    st.markdown("---")
    st.markdown("### üìä Datasets utilis√©s")

    datasets = load_datasets()
    for name, df in datasets.items():
        st.write(f"**{name.upper()}** : {df.shape[0]} lignes √ó {df.shape[1]} colonnes")


elif page == "üìä Task 1: D√©couverte Algorithmique":
    st.header("Task 1 : D√©couverte Algorithmique des FDs")

    datasets = load_datasets()

    # S√©lection du dataset
    dataset_name = st.selectbox("Choisir un dataset :", list(datasets.keys()))
    df = datasets[dataset_name]

    st.subheader(f"Aper√ßu de {dataset_name.upper()}")
    st.dataframe(df.head(10))

    col1, col2 = st.columns(2)
    with col1:
        min_validity = st.slider("Validit√© minimum (%)", 80, 100, 90)
    with col2:
        max_lhs = st.slider("Taille max du LHS", 1, 3, 2)

    if st.button("üîç D√©couvrir les FDs", type="primary"):
        with st.spinner("Analyse en cours..."):
            fds = discover_fds(df, min_validity=min_validity, max_lhs_size=max_lhs)

        st.success(f"‚úÖ {len(fds)} FDs d√©couvertes !")

        if fds:
            fds_df = pd.DataFrame(fds)
            fds_df = fds_df.sort_values('validity', ascending=False)

            st.subheader("FDs d√©couvertes")
            st.dataframe(fds_df[['fd_string', 'validity', 'violations']], use_container_width=True)

            # Stats
            col1, col2, col3 = st.columns(3)
            with col1:
                exact = len([f for f in fds if f['validity'] == 100])
                st.metric("FDs exactes (100%)", exact)
            with col2:
                approx = len([f for f in fds if f['validity'] < 100])
                st.metric("FDs approximatives", approx)
            with col3:
                st.metric("Total", len(fds))


elif page == "ü§ñ Task 2: Analyse LLM":
    st.header("Task 2 : Analyse S√©mantique avec LLM")

    st.markdown("""
    Le LLM (Claude 3 Haiku) √©value chaque FD et attribue :
    - Un **score** de 0 √† 10
    - Une **cat√©gorie** : key, business_rule, derived, accidental, meaningless
    """)

    # Charger les r√©sultats pr√©-calcul√©s
    results_path = 'results/task4_hybrid_results.json'
    if os.path.exists(results_path):
        with open(results_path, 'r') as f:
            results = json.load(f)

        st.subheader("R√©sultats de l'analyse s√©mantique")

        results_df = pd.DataFrame(results)

        # Afficher par cat√©gorie
        categories = results_df['category'].unique()

        for cat in ['key', 'business_rule', 'derived', 'accidental', 'meaningless']:
            if cat in categories:
                cat_df = results_df[results_df['category'] == cat]

                if cat == 'key':
                    st.success(f"üîë **{cat.upper()}** ({len(cat_df)} FDs)")
                elif cat == 'business_rule':
                    st.info(f"üìã **{cat.upper()}** ({len(cat_df)} FDs)")
                elif cat == 'derived':
                    st.warning(f"üîó **{cat.upper()}** ({len(cat_df)} FDs)")
                else:
                    st.error(f"‚ùå **{cat.upper()}** ({len(cat_df)} FDs)")

                for _, row in cat_df.iterrows():
                    st.write(f"  ‚Ä¢ `{row['fd']}` - Score: {row['semantic_score']}/10 - {row.get('reason', '')}")
    else:
        st.warning("R√©sultats non disponibles. Ex√©cutez d'abord le notebook Task 4.")

    st.markdown("---")
    st.subheader("üí° Cat√©gories expliqu√©es")

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        | Cat√©gorie | Description |
        |-----------|-------------|
        | **key** | Cl√© primaire / identifiant unique |
        | **business_rule** | R√®gle m√©tier significative |
        | **derived** | Attribut calcul√© ou d√©riv√© |
        """)
    with col2:
        st.markdown("""
        | Cat√©gorie | Description |
        |-----------|-------------|
        | **accidental** | Corr√©lation sans causalit√© |
        | **meaningless** | Pas de sens s√©mantique |
        """)


elif page == "üìâ Task 3: √âchantillonnage":
    st.header("Task 3 : √âchantillonnage et Faux Positifs")

    st.markdown("""
    ## ‚ö†Ô∏è Le probl√®me de l'√©chantillonnage

    Une FD peut sembler **vraie sur un √©chantillon** mais √™tre **fausse sur le dataset complet**.
    """)

    # Charger les r√©sultats
    results_path = 'results/task3_validation_results.csv'
    if os.path.exists(results_path):
        results_df = pd.read_csv(results_path)

        # Stats
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total FDs test√©es", len(results_df))
        with col2:
            tp = len(results_df[results_df['category'] == 'true_positive'])
            st.metric("‚úÖ Vraies positives", tp)
        with col3:
            fp = len(results_df[results_df['category'] == 'false_positive'])
            st.metric("‚ùå Faux positifs", fp, delta=f"{fp/len(results_df)*100:.1f}%", delta_color="inverse")
        with col4:
            fn = len(results_df[results_df['category'] == 'false_negative'])
            st.metric("Fausses", fn)

        st.markdown("---")
        st.subheader("üö® Les Faux Positifs d√©tect√©s")

        false_positives = results_df[results_df['category'] == 'false_positive']

        if len(false_positives) > 0:
            for _, fp in false_positives.iterrows():
                st.error(f"""
                **FD : `{fp['fd']}`**
                - Dataset : {fp['dataset']}
                - Validit√© √©chantillon : **{fp['approx_sample']:.1f}%** ‚úì
                - Validit√© complet : **{fp['approx_full']:.1f}%** ‚úó
                - Violations cach√©es : {fp['violations_full']}
                """)

        st.markdown("---")
        st.subheader("üìä Tableau complet")
        st.dataframe(results_df[['dataset', 'sample_type', 'fd', 'holds_sample', 'holds_full', 'category']])
    else:
        st.warning("R√©sultats non disponibles. Ex√©cutez d'abord le notebook Task 3.")

    st.markdown("---")
    st.info("""
    ### üí° Le√ßon cl√©

    > **"L'√©chantillonnage cr√©e des HYPOTH√àSES, pas des V√âRIT√âS.
    > Toujours valider sur le dataset complet !"**
    """)


elif page == "üîÑ Task 4: Pipeline Hybride":
    st.header("Task 4 : Pipeline Hybride Algorithme + LLM")

    st.markdown("""
    ## Architecture du Pipeline

    ```
    Dataset ‚Üí [Algorithme] ‚Üí FDs candidates ‚Üí [LLM] ‚Üí FDs significatives
                  ‚Üì                              ‚Üì
              Validit√©                      Score 0-10
              technique                     + Cat√©gorie
                                                ‚Üì
                                         Score Hybride
    ```
    """)

    st.latex(r"Score_{hybride} = \frac{Validit√©_{technique}/10 + Score_{s√©mantique}}{2}")

    # Charger les r√©sultats
    results_path = 'results/task4_hybrid_results.csv'
    if os.path.exists(results_path):
        results_df = pd.read_csv(results_path)
        results_df = results_df.sort_values('hybrid_score', ascending=False)

        st.markdown("---")
        st.subheader("üìä R√©sultats du Pipeline Hybride")

        # Classification
        col1, col2, col3 = st.columns(3)
        with col1:
            sig = len(results_df[results_df['hybrid_score'] >= 7])
            st.metric("üåü Significatives (‚â•7)", sig)
        with col2:
            useful = len(results_df[(results_df['hybrid_score'] >= 5) & (results_df['hybrid_score'] < 7)])
            st.metric("üëç Utiles (5-7)", useful)
        with col3:
            ignore = len(results_df[results_df['hybrid_score'] < 5])
            st.metric("‚ùå √Ä ignorer (<5)", ignore)

        st.markdown("---")

        # Tableau des r√©sultats
        st.subheader("üèÜ Classement des FDs")

        display_df = results_df[['dataset', 'fd', 'technical_validity', 'semantic_score', 'hybrid_score', 'category']].copy()
        display_df['technical_validity'] = display_df['technical_validity'].round(1)
        display_df['hybrid_score'] = display_df['hybrid_score'].round(2)

        st.dataframe(display_df, use_container_width=True)

        # Stats par cat√©gorie
        st.markdown("---")
        st.subheader("üìà Score moyen par cat√©gorie")

        cat_stats = results_df.groupby('category')['hybrid_score'].agg(['count', 'mean']).round(2)
        cat_stats.columns = ['Nombre', 'Score moyen']
        st.dataframe(cat_stats.sort_values('Score moyen', ascending=False))
    else:
        st.warning("R√©sultats non disponibles. Ex√©cutez d'abord le notebook Task 4.")


elif page == "üìà R√©sultats Finaux":
    st.header("üìà R√©sultats Finaux et Conclusions")

    st.markdown("""
    ## üéØ Synth√®se du Projet
    """)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        ### Ce qu'on a appris :

        1. **Les algorithmes trouvent TOUT** mais sans discernement
        2. **Les LLMs comprennent le sens** mais peuvent halluciner
        3. **L'√©chantillonnage est dangereux** : 9% de faux positifs
        4. **L'approche hybride** combine le meilleur des deux
        """)

    with col2:
        st.markdown("""
        ### Chiffres cl√©s :

        | M√©trique | Valeur |
        |----------|--------|
        | FDs d√©couvertes (algo) | 331 |
        | FDs candidates (hybride) | 16 |
        | FDs significatives | **12** |
        | R√©duction du volume | **96%** |
        | Faux positifs √©chantillonnage | **9%** |
        """)

    st.markdown("---")

    st.success("""
    ## üí° Message Final

    > **"La d√©couverte de d√©pendances fonctionnelles significatives n√©cessite une approche hybride
    > combinant la pr√©cision des algorithmes et l'intelligence s√©mantique des LLMs."**
    """)

    st.markdown("---")
    st.markdown("### üîó Liens")
    st.markdown("- [GitHub Repository](https://github.com/keita223/Projet_Data_Quality)")
    st.markdown("- [Rapport complet](Rapport_Projet_Data_Quality.md)")

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("**Projet M2 IASD**")
st.sidebar.markdown("Paris-Dauphine 2026")
